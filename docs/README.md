### 编译选项
除了cmake标准编译选项外，libatbus还提供一些额外选项

+ ATBUS_MACRO_BUSID_TYPE: busid的类型(默认: uint64_t)，建议不要设置成大于64位，否则需要修改protocol目录内的busid类型，并且重新生成协议文件
+ GTEST_ROOT: 使用GTest单元测试框架
+ BOOST_ROOT: 设置Boost库根目录
+ PROJECT_TEST_ENABLE_BOOST_UNIT_TEST: 使用Boost.Test单元测试框架(如果GTEST_ROOT和此项都不设置，则使用内置单元测试框架)


## 开发文档
### 目录结构说明

+ third_party: 外部组件（不一定是依赖项）
+ docs: 文档目录
+ include: 导出lib的包含文件（注意不导出的内部接口后文件会直接在src目录里）
+ project: 工程工具和配置文件集
+ protocol: 协议描述文件目录
+ sample: 使用示例目录，每一个cpp文件都是一个完全独立的例子
+ src: 源文件和内部接口申明目录
+ test: 测试框架及测试用例目录

### 关于 #pragma once
由于目标平台和环境的编译器均已支持 #pragma once 功能，故而所有源代码直接使用这个关键字，以提升编译速度。

详见:[pragma once](http://zh.wikipedia.org/wiki/Pragma_once) 


### 内存通道设计
单多写状态
```
                   ▼       ▼
-------------------WWWW####WWWW###----------------
                   △
```
|长度|           节点头结构           |       说明       |
|---|------------------------------|------------------|
|1B |           标记 flag           |是否写完、是否是头节点|
|1B |        写权限（原子操作）        |       |
|4B |        首读时间（毫秒）          |最大容忍误差是49天|


**内存通道结构(内存和共享内存)**

所有消息对齐到size_t的大小
|4K通道头|数据节点头*数据节点个数|数据区|

```cpp
// 通道头
struct mem_channel {
    // 数据节点
    size_t node_size;  /** 每个节点的size **/
    size_t node_size_bin_power; // (用于优化算法) node_size = 1 << node_size_bin_power
    size_t node_count; /** 数据节点个数 **/

    // [atomic_read_cur, atomic_write_cur) 内的数据块都是已使用的数据块
    // atomic_write_cur指向的数据块一定是空块，故而必然有一个node的空洞
    // c11的stdatomic.h在很多编译器不支持并且还有些潜规则(gcc 不能使用-fno-builtin 和 -march=xxx)，故而使用c++版本
    volatile std::atomic<size_t> atomic_read_cur;   // std::atomic也是POD类型
    volatile std::atomic<size_t> atomic_write_cur;  // std::atomic也是POD类型

    // 第一次读到正在写入数据的时间
    uint32_t first_failed_writing_time; /** 第一次读到正在写节点的时间，用于跳过错误写 **/

    volatile std::atomic<uint32_t> atomic_operation_seq; // 操作序列号(用于保证只有一个接收者)

    // 配置
    mem_conf conf;
    size_t area_channel_offset; /** 地址偏移: channel **/
    size_t area_head_offset;    /** 地址偏移: 数据节点头 **/
    size_t area_data_offset;	/** 地址偏移: 数据区 **/
    size_t area_end_offset;		/** 地址偏移: 使用的缓冲区尾部 **/

    // 统计信息
    size_t block_bad_count; 	// 读取到坏块次数
    size_t block_timeout_count; // 读取到写入超时块次数
    size_t node_bad_count; 		// 读取到坏node次数
};

// 配置数据结构
struct mem_conf {
    size_t protect_node_count;	/** 保护节点个数：用于降低冲突概率 **/
    size_t protect_memory_size;	/** 保护内存大小：用于降低冲突概率 **/
    uint64_t conf_send_timeout_ms;	/** 发送超时阀值：用于降低冲突概率 **/

    // TODO 接收端校验号(用于保证只有一个接收者)
    volatile std::atomic<size_t> atomic_receiver_identify;
};
```

**写数据步骤：**

1. 获取写游标，比较读游标，判断是否有空间
2. 分配操作序号
3. 顺序设置操作序号，交换0序号块，失败则返回空间不足
4. 逆序写数据，设置写完状态


**读数据步骤：**

1. 获取读游标，比较写游标，判断是否有数据
2. 分配操作序号
2. 获取第一个节点是否准备完成状态
3. 如果不是完成状态尝试设置首读时间，如果首读超出阀值则认为写错误。此时reset脏节点（非头且不到写游标节点）后移动读游标


**关于冲突：**

1. **读-读冲突：**只考虑单点读，没有这个问题。
2. **读-写冲突：**head有写完毕标记位，当写数据块准备完毕时才开始读。
3. **写-写冲突：**写游标是原子操作，每个节点写缓冲区独立。如果两个节点同时写一个块，则只有一个能写成功。如果写序列中任意块写失败，则整体返回空间不足，写失败。（防止读失败后释放的内存被重新写导致写冲突）
4. **写进程崩溃：**会产生赃数据块，即写完标记永远是未写完。这时候可以利用上上面提到的第一次读取时间。如果是0，则取当前时间赋值，否则如果超出容忍值，就视为赃数据块。取时间可以使用clock函数（Linux下实测每次执行消耗约160ns），也可以用汇编直接提取CPU时钟。一般情况下系统应该在数百次读取无数据后休眠至少一个时间片的时间(Linux下一般最少有4ms)，这时候写进程还没写完基本可以认为是出现赃数据。
5. **读进程崩溃：**移动读游标是最后的操作，下次启动时可以继续，不会丢失数据

**写-读失败-写覆盖问题：**

有一个目前无法解决的是**写-读失败-写覆盖**的问题。这个问题比较难处理，而且发生情况很少。为了这个偶现的问题增加锁和复杂的错误处理逻辑我认为是很不值得的，所以这里采用一些措施来提早发现问题。

+ **第一个措施**是增加一个保护区，当剩于空间不足某个阀值时直接返回空间不足
+ **第二个措施**是增加一个简单的校验码，当校验不通过时返回错误

在设置合理的情况下这两个措施基本能保证数据不出错（如果设置合理，再出错的概率按某人的说法就是，硬件也会出错坏掉的啊）

**共享内存通道压力测试**
1个读进程，5个写进程
读进程满负荷运行3小时，接收数据3390712433次，接收数据12933GB，出现9次数据坏块错误，无数据校验错误
出错率低于3.7亿分之一


### 网络通道设计

网络消息收发走socket协议，然而由于**socket是一对一**的，所以需要对消息接收做一个汇总操作。另外网络通道由于不是使用预分配的内存，所以还需要一个回收操作。

消息接收的汇总聚合需要IO复用的支持，为了简化网络层跨平台适配，我们直接使用libuv。

另外网络通道和内存通道还有几个不同的地方:

1. 网络通道并不是一个真实的通道，所以**逻辑上接收通道（通过init创建）不能发送数据**，**发送通道（通过attach创建）不能用于接收数据**。这种情况下，获取数据时需要带回socket标识，用以做安全性控制。
2. 由于接收通道和发送通道socket是分离的，意味着一个节点可能会有多个接收通道。
3. 由于每条连接的接收端不一样，所以每个接收socket需要有自己的缓冲区。
4. 需要处理被动断线和断线重连的问题。而断线重连时也需要区分连接是否能成功的不同逻辑。
5. 发送接口除了发送成功和发送失败以外还有一个**发送中的状态**。（由于MTU分片，有些数据一次发不完，需要一点一点地发）

根据以上特点，主要设计思路如下:

1. 每个**数据节点**拥有一个libuv的context，用于异步分发fd事件。
2. 每个**数据节点**需要有一个连接池，连接池内的连接需要保存一些连接数据，包含发送缓冲区、接收缓冲区、状态等等。
3. 连接的发缓冲区有两种形式，一种是固定缓冲区（固定大小，固定个数）。另一种是动态缓冲区，动态缓冲区会频繁malloc，所以最好要使用jemalloc之类的内存分配器，并且动态缓冲区是一个链表，要有最大上限。
4. 数据发送、接收需要统计数据。
5. 断开连接时需要能够通过析构逻辑释放所有缓冲区和待发送项，并且回调失败接口。
6. 网络出现问题（包含超时、断开等）需要进行重连逻辑，但是重连逻辑需要有重试次数限制。立即重试失败则会有定时重试机制。
7. 网络断开事件中要清理节点的连接信息。
8. 如果数据很小能直接发掉，就直接发送（直接进入系统socket缓冲区），不需要过缓冲区（当然当前缓冲区必须为空）。发送接口防止多线程要加锁（先全加自旋锁，后面可以考虑抽空移植BOOST中对线程支持的判定，有多线程支持时加自旋锁）。

```cpp
// Sock通道状态
struct sock_status_t {
    enum type {
        INIT = 0,
        CONNECTING,
        CONNECTED,
    };
};

// Sock通道头
struct io_stream_channel {
    std::string host;           // 主机地址
    uint16_t    port;           // 端口
    int         fd;             // socket设备描述符/HANDLE
    int         status;         // 状态

    // 数据区域
    buffer_manager write_buffers;     // 写数据缓冲区(两种Buffer管理方式，一种动态，一种静态)
    buffer_manager read_buffers;      // 读数据缓冲区(两种Buffer管理方式，一种动态，一种静态)
    
    // 回调函数
    connect_callback_t on_connect;
    disconnect_callback_t on_disconnect;
    recv_callback_t on_recv;
    
    // 统计信息
    size_t block_bad_count; 	// 读取到坏块次数
    size_t block_timeout_count; // 读取到写入超时块次数
    size_t node_bad_count; 		// 读取到坏node次数
};
```

### 数据节点

数据节点从接收通道上从逻辑区分可以有**命令通道**、**数据通道**。控制命令优先走**命令通道**，数据收发走**数据通道**。并且每种逻辑通道都可以是上面提到的任意N种类型。不过**libatbus**对通道的收发类型不做明确限制，而是根据协议来判定。

**libatbus**会把IO流通道，accept的节点作为命令通道发送节点。数据通道另外发起连接。而对于内存或共享内存节点。不区分数据通道或命令通道。

数据节点的发送通道可以多种多样，但是KEY都是节点的ID。Value里包含连接信息，并且能根据连接信息来判定怎么建立连接或者如何发送数据。

对单个数据节点的操作必须是单线程的。包含节点更新、获取、查找等。

另外libatbus不规定通信模式（不像zeromq一样必须指定一种通信模式）。所以基本没有回包一说。但是因为存在网络延迟发送和发送过程，所以会出现发送失败的问题。

然而，在最极端的条件下，即便TCP连接的底层接口返回发送成功，也不能保证对端能正确收到（因为是异步接口并且底层可能发送到一半连接断开并且重试失败），能保证的只是对方收到的情况下的顺序和内容。
为了尽可能的抛出网络问题，调用发送接口时。我们在尝试重连的时候直接向上层直接返回错误。其他情况下， EAGAIN和EWOULDBLOCK、EINTR则直接重试，其他错误直接返回错误。

连接协议使用类似zeromq的方式。具体实施规则如下:

+ TCP网络连接: ipv4://IP:端口, ipv6://IP:端口, dns://域名或IP:端口
+ Unix Socket连接: unix://文件名路径 （如果是绝对路径，比如/tmp/atbus.sock的完整路径是 unit:///tmp/atbus.sock）
+ 共享内存连接: shm://共享内存Key
+ 堆内存连接: mem://名称

内部协议类型:
1. 转发协议
2. 节点树同步协议
3. 注册协议
4. 建立连接协议
5. Ping协议
